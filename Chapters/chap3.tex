%!TEX root = /Users/admin/Desktop/Documents/Academic/MA 470 -THESIS/THESIS/thesis.tex
\chapter{The Synchronous \Picalc}\label{sync_and_express}
In \refsec{spisyntax}, we introduced a version of the \picalc\ which we said was made \emph{asynchronous} by its use of non-blocking send operations.  
Rather than allowing an operation to happen after a sent value has been received by some other process, a sending process simply ends.  
In \refex{exsynchronous} we gave a straightforward method of simulating synchronous sending in the asynchronous \picalc\ using acknowledgement channels.  
Also absent in our calculus was the non-deterministic choice operator, also known as the summation operator.  
We gave a method of simulating this in \refex{exsummation}.

We have already had a taste of the expressive power of the synchronous \picalc\ in Chapter \ref{Introduction}'s mobile phone network example.  In fact, the original calculus given by Milner, Parrow and Walker was synchronous and included the two operators -- summation and synchronous sending -- that we omitted in our asynchronous \picalc.  

Our original calculus featured synchronous receiving, but we now make sending synchronous as well, allowing a process to be performed after the output has been consumed by some other receiver.  
Hence, in the synchronous \picalc, processes can be \refmargin{guarded}guarded by both receiving \emph{and} sending.  


If we have a group of guarded processes joined by the summation operator, only one of those will be processes will be `chosen' (non-deterministically)\index{non-determinism} to be executed.  
The rest will simply terminate without having any effect.  
The power of the synchronous \picalc\ comes when we have another group of summed processes, running in parallel, something like: 
\[
	\ssend{c}{}P_1 + \ssend{d_1}{}P_2 + \receive{d_2}{}P_3 \comp \receive{c}{}R_1 + \ssend{d_3}{}R_2
\]
In the above isolated system, we actually \emph{can} know which of the processes will be chosen to run, despite the non-deterministic nature of the choice operator.  
Because both sends and receives are guarded, the $P_k$'s and $R_k$'s above can only run when their respective transmission guards complete.  
Hence, only $P_1$ and $R_1$ will be allowed to run since sending on $c$ is the only transmission with a `matching' reception on the same channel.  
Now imagine a slightly different system:
\[
	\ssend{c}{}P_1 + \ssend{d}{}P_2 \comp \receive{c}{}R_1 + \receive{d}{}R_2
\]
Here we have \emph{two} transmissions with matches, on $c$ and on $d$.  
Hence, we cannot make a guarantee about which processes are chosen.  
However, we \emph{can} say that if $P_1$ executes on the left side, $R_1$ will be chosen on the right side.  
We can say the same for $P_2$ and $R_2$.  
There is a silent, implicit sort of communication that happens between groups of parallel processes when a non-deterministic choice is made among them.  
This is a powerful feature which comes with the special operators of the synchronous \picalc.  

Now that we have gotten an idea of the synchronous \picalc' unique properties, we will turn in the next section to its formal syntax and rules of computation.

\section{The Synchronous \Picalc}\label{Synchronous picalc}

\begin{insettable}
\begin{center}
\begin{tabular}{r l l}
\multicolumn{3}{c}{\emph{Action Prefixes}}\\
$\pi :=$  & $\send{n}{\tuple{V}}$ & Send\\
&$\receivenodot{n}{\tuple{X}}$ & Receive\\

&\\

\multicolumn{3}{c}{\emph{Process terms}}\\
$R :=$ & $\displaystyle\sum_{i\in \{1,..,n\}} \pi_i.R_i$ &\multirow{2}{*}{Summation ($n \in \mathbb{Z}$)}\\[20pt]
&$R_1 \comp R_2$ & Composition\\
&$\new{n}R$ & Restriction\\
&$\pif{v_1 = v_2}\pthen R_1 \pelse R_2$ & Matching\\
&$\rec{x} R$ & Recursion\\
&\pstop & Termination\\
&\\
\end{tabular}
\caption{\emph{Terms in the synchronous \picalc}}\label{spicalcterms}
\end{center}
\end{insettable}

Note the important difference between \reffig{apicalcterms} and \reffig{spicalcterms}.  
First, we have grouped sending and receiving together as \emph{action prefixes}. 
These prefixes are made available to processes via the summation operator. 

Consider the term:
\[
	\sum_{i\in \{1,..,n\}} \pi_i.R_i
\]
The notation $\pi_i.R_i$ requires that the action $\pi_i$ happen before the guarded process $R_i$ can be executed.  
If $R_a$ is executed in this way, then for all $j\in \{1,...,n\},j \neq a$, the capabilities for both the action $\pi_j$ and the execution of $R_j$ are lost.  
In other words, the summation ensures that only one of $n$ guarded processes will be executed, providing a branching behavior in the logic of a term. 
Which of these branches is picked depends on which action prefix capability is exercised first.  As we saw above, we cannot always guarantee which of the action capabilities will be exercised, so we say that the summation operator is non-deterministic.\index{non-determinism}

For the cases that $n=1$ and $n=2$, we will use the notation $\pi.R$ and $\pi_1.R_1 + \pi_2.R_2$, respectively.  Notice that the $n=1$ case is the equivalent of the process terms of our asynchronous calculus.
To accommodate for the new operator, we add to our congruence relation given in \refdef{Structural Equivalence} that summation is commutative (S-SUM-COMM) and associative (S-SUM-ASSOC).

The summation case of $n=0$ is what is meant by our $\pstop$ termination process.  
It behaves just as it did in our asynchronous calculus.
We also add to our congruence relation the following trivially true fact, which we call (S-SUM-ID)
\begin{align*}
	R + 0 \sequiv R
\end{align*}

Note also that in our action prefixes, we have made sending a\refmargin{guarded}guarded operation, which means that in the term
\[
	\ssend{n}{\tuple{V}}R
\]
$R$ will not execute until some other process receives $\tuple{V}$ along $n$.  
Receiving is also guarded, as in the asynchronous version.  

\begin{insettable}
\begin{center}
	\begin{align*}
		R + Q\ &\   \sequiv\ Q + R && \text{\tiny{(S-SUM-COMM)}}\\
		(P + Q) + R\ &\   \sequiv\ P + (Q + R) && \text{\tiny{(S-SUM-ASSOC)}}\\
		R + 0\ &\   \sequiv\ R && \text{\tiny{(S-SUM-ID)}}\\
		P \comp Q\ &\  \sequiv\  Q \comp P && \text{\tiny{(S-COMP-COMM)}}\\
	 	(P \comp Q) \comp R\ &\ \sequiv\ P \comp (Q \comp R) && \text{\tiny{(S-COMP-ASSOC)}}\\
		P \comp \pstop\ &\ \sequiv\ P && \text{\tiny{(S-COMP-ID)}}\\
		\new{c} \pstop\ &\ \sequiv\ \pstop && \text{\tiny{(S-REST-ID)}}\\
		\new{c}\new{d} P \ &\ \sequiv\ \new{d}\new{c} P && \text{\tiny{(S-REST-COMM)}}\\
		\new{c}(P \comp Q)\ &\ \sequiv\  P \comp \new{c}Q\text{, if } c\not\in fi(P) && \text{\tiny{(S-REST-COMP)}}
	\end{align*}
	\caption{\emph{Structural equivalence axioms in the synchronous \picalc}}\label{spicalcaxioms}
\end{center}\end{insettable}

\section{Computation in the Synchronous \Picalc}
We are now ready to give a description of the computation behavior.  
As we might expect, it does not differ hugely from computation in the asynchronous calculus.  
In the reduction rules, the only changes are to make room for the summation operator.
Because only one process gets chosen from a group of summed processes, it is not enough to have two `matching' send and receive action prefixes in a summation.
Instead, we need one of each running in parallel.  This behavior is described by (R-SYNC).  
It expresses the commutation step where the processes have transmitted their value.  Hence, we perform the appropriate substitution to the receiving process, run the guarded processes and terminate all the other terms in the sum.
\begin{insettable}
\begin{center}\begin{tabular}{rll}
	$\ssend{c}{\tuple{V}} P + Q \comp \receive{c}{\tuple{X}}R + B$\ &\  $\pred\  P \comp R\subst{\tuple{V}}{\tuple{X}}$ & \tiny{(R-SYNC)}\\
	$\rec{x}R$\ &\  $\pred\  R\subst{\rec{x}R}{x}$ & \tiny{(R-REP)}\\
	$\pif{v = v}\pthen P \pelse Q$\ &\ $\pred\ P$ & \tiny{(R-EQ)}\\
	$\pif{v_1 = v_2}\pthen P \pelse Q$\ &\ $\pred\ Q$ \ \ (where $v_1\neq v_2$)& \tiny{(R-NEQ)}\\
	\multicolumn{2}{c}{\hspace{4.5em}$\underline{P\sequiv P', P \pred Q, Q\sequiv Q'}$} & \multirow{2}{*}{\tiny{(R-STRUC)}}\\
	\multicolumn{2}{c}{\hspace{4.5em}$P'\pred Q'$}
\end{tabular}
\caption{\emph{Reduction rules for the synchronous \picalc}}\label{spireducs}
\end{center}
\end{insettable}

\begin{example}{syncembedding}
	It is not hard to show that the synchronous \picalc\ can model the asynchronous version.  
To see why, first note that asynchronous sending can be encoded simply by $\ssend{n}{\tuple{V}}\pstop$.  
This we will abbreviate with the familiar notation $\send{n}{\tuple{V}}$.  
As we noted above, the summation notation allows for a single guarded process.  
If we limit ourselves to these single summations and limit all sending to be of the form $\ssend{n}{\tuple{V}}\pstop$, then we have the asynchronous \picalc.  
To see why the reduction semantics are compatible, consider the following which shows that (R-COMM) of \refdef{Reduction} can be considered to be a special case of (R-SYNC):\\
	
	\begin{tabular}{rlr}
				\ssend{c}{\tuple{V}} \pstop + \pstop \comp \receive{c}{\tuple{X}}R + \pstop\ \ &\ \ \sequiv\ \ \  \ssend{c}{\tuple{V}}\pstop \comp \receive{c}{\tuple{X}}R, & \text{\tiny{(S-SUM-ID)}}\\
		\ssend{c}{\tuple{V}}\pstop \comp \receive{c}{\tuple{X}}R\ &\  \pred\  (\pstop \comp R)\subst{\tuple{V}}{\tuple{X}}, & \text{\tiny{(R-SYNC)}}\\
		\multicolumn{2}{c}{\hspace{44pt}$\underline{(\pstop \comp R)\subst{\tuple{V}}{\tuple{X}}\ \ \ \ \sequiv\ \ \ R\subst{\tuple{V}}{\tuple{X}}}$} & \text{\tiny{(S-COMP-ID)}}\\
		\send{c}{\tuple{V}} \comp \receive{c}{\tuple{X}}R\ &\  \pred\  R \subst{\tuple{V}}{\tuple{X}} & \text{\tiny{(R-STRUC)}}\\
	\end{tabular}\\
	
	It will be obvious from our presentation of synchronous action rules below that they needn't be shown to be a general version of the asynchronous rules -- they are compatible simply by ignoring the extra rule and using our $\ssend{n}{\tuple{V}}\pstop$ encoding of sending.
\end{example}

Neither do the action rules for the synchronous \picalc\ differ significantly from \hyperref[apiactionrules]{those} in the asynchronous version.  
As we would expect, (A-OUT) has been modified to express synchronous sending. 
It now evolves over output to a process $R$ and not simply to the termination process $\pstop$:
\[
	\ssend{c}{\tuple{V}}R \evolves{\sends{c}{\tuple{V}}} R	
\]

We have also added a new rule, (A-SUM), allows for the summation operator in a manner that is similar to the way (A-COMP) allows for the composition operator.
\begin{center}
	$\underline{P_i \evolves{\pi_i} P_i'}$\\
	$\displaystyle\sum_{i\in \{1,..,n\}}\pi_i.P_i \evolves{\pi_i} P_i'$\\
\end{center}
(A-SUM) expresses that if we know a process evolves on its own then we can infer that it will evolve in a summation.  It says, for example, that if some process $\pi_1.P$ is able to evolve to $P'$ over some action $\pi_1$, then $\pi_1$ will also cause $\pi_1.P+\pi_2.Q$ to evolve, over the same $\pi_1$, to the choice of $P'$.
Notice that unlike (A-COMP), (A-SUM) does not need to be careful about capturing names. 
Since there no other processes are run by the summation, we needn't worry about causing a naming issue when we possibly export terms by running the action $\alpha$.

\begin{insettable_wide}
	\begin{center}\begin{tabular}{rllll}
 		$\receive{c}{\tuple{X}}R$ & \evolves{\receives{c}{\tuple{V}}} & $R$\subst{\tuple{V}}{\tuple{X}} & & \tiny{(A-IN)}\\
		$\ssend{c}{\tuple{V}}R$ & \evolves{\sends{c}{\tuple{V}}} & $R$ & & \tiny{(A-OUT)}\\
		$\rec{x}R$ & \evolves{\tau} & $R\subst{\rec{x}R}{x}$ & & \tiny{(A-REP)}\\
		$\pif{v=v} \pthen P \pelse Q$ & \evolves{\tau} & $P$ & & \tiny{(A-EQ)}\\[10pt]
		$\pif{v_1=v_2} \pthen P \pelse Q$ & \evolves{\tau} & $Q$ & $v_1 \neq v_2$ & \tiny{(A-NEQ)}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P_i \evolves{\pi_i} P_i'}$} &  & \multirow{2}{*}{\tiny{(A-SUM)}}\\
		\multicolumn{3}{c}{$\displaystyle\sum_{i\in \{1,..,n\}}\pi_i.P_i \evolves{\pi_i} P_i'$}\\[18pt]
		
		\multicolumn{3}{c}{$\underline{P \evolves{\pi} P'}$} & \multirow{2}{*}{\footnotesize{$\textstyle bn(\alpha) \cap fn(Q) = \emptyset$ }} & \multirow{2}{*}{\tiny{(A-COMP)}}\\
		\multicolumn{3}{c}{$P\comp Q \evolves{\alpha} P'\comp Q$}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P \evolves{\alpha} P'}$} & \multirow{2}{*}{\footnotesize{$\textstyle b \not \in n(\alpha)$ }} & \multirow{2}{*}{\tiny{(A-REST)}}\\
		\multicolumn{3}{c}{$\new{b} P \evolves{\alpha} \new{b} P'$}\\[10pt]

		\multicolumn{3}{c}{$\underline{P\evolves{\exports{\tuple{B}}\sends{c}{\tuple{V}}} P'}$} & \multirow{2}{*}{\footnotesize{$n \neq c, n\in \tuple{V}$ }}& \multirow{2}{*}{\tiny{(A-OPEN)}}\\
		\multicolumn{3}{c}{$\new{n}P \evolves{\exports{n,\tuple{B}}\sends{c}{\tuple{V}}} P'$}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P\evolves{\receives{c}{\tuple{X}}} P',\ Q \evolves{\exports{\tuple{B}}\sends{c}{\tuple{V}}} Q'}$} & \multirow{2}{*}{\footnotesize{$\textstyle \exports{\tuple{B}}\cap fn(P) = \emptyset$ }} & \multirow{2}{*}{\tiny{(A-COMM)}}\\
		\multicolumn{3}{c}{$P\comp Q \evolves{\tau} \new{\tuple{B}}(P'\comp Q')$}\\[10pt]
	\end{tabular}	
	\caption{\emph{Action rules for the synchronous \picalc}}\label{spiacts}
\end{center}
\end{insettable_wide}

\section{Extended Example: Leader Elections}\label{secleaderelecs}
Leader elections, a classic problem in distributed systems, are a good example of the power of the synchronous \picalc.
A leader election is a system where a group of processes, each with a unique identifier (via integers, perhaps), must agree on a `leader' process identification in a finite amount of time.
The processes vote on a process to be their leader by sending an integer-valued `vote' $v_i$ on a given output channel $o$.  
Ideally, we want each of the processes to operate using the same `program', without any preference or priority hard-coded into that program.

One means of specifying when two processes' programs are the same is by the concept of \defmargin{symmetry}.
We say that two processes are symmetric if they are equivalent under structural equivalence and a systematic renaming of their identifiers.
To better understand what is meant by `a systematic renaming', assume that each process $p_i$, channel name $c_i$, and vote $v_i$ has a unique identifier $i \in \{1,..,n\}$.
Now suppose we have an isomorphism $\sigma$, given by following recursive definition, that maps these identifiers to other identifiers in the space $\{1,...,n\}$.
\begin{insettable_wide}
	\begin{center}
		\begin{tabular}{r l l}
		\multicolumn{3}{c}{\emph{Action Prefixes}}\\
			$\sigma(\sends{c_i}{v_i})$ &$= $ &$ \sends{c_{\sigma(i)}}{v_{\sigma(i)}}$\\
			$\sigma(\receives{c_i}{v_i})$ &$=$ &$ \receives{c_{\sigma(i)}}{v_{\sigma(i)}}$\\[18pt]
		\multicolumn{3}{c}{\emph{Processes}}\\
			$\sigma(\displaystyle\sum_{i\in \{1,..,n\}} \pi_i.R_i)$ &$=$ &$ \displaystyle\sum_{i\in \{1,..,n\}} \sigma(\pi_i).\sigma(R_i)$\\
			$\sigma(R_1 \comp R_2)$ &$=$ &$ \sigma(R_1)\comp \sigma(R_2)$\\
			$\sigma(\new{n}R)$ & $=$ &$  \new{n}\sigma(R)$\\
			$\sigma(\pif{v_i = v_j}\pthen R_1 \pelse R_2)$ &$=$ &$ \pif{v_{\sigma(i)} = v_{\sigma(j)}}\pthen \sigma(R_1) \pelse \sigma(R_2)$\\
			$\sigma(\rec{x} R)$ & $=$ &$\rec{x}\sigma(R)$\\
			$\sigma(\pstop)$ & $=$ & $\pstop$\\
		\end{tabular}
	\end{center}
	\caption{\emph{Rules for applying $\sigma$}}\label{sigmarules}
\end{insettable_wide}
Using these rules, we have a systematic function $\sigma$ for renaming identifiers in a process.  When a system of processes running in parallel are all symmetric to one another, we will say that the system is symmetric.
For example, consider the following symmetric system:
\begin{align}
	P_0\comp P_1 \pdef \ssend{c_0}{} \send{o}{0} + \receive{c_1}{} \send{o}{1} \comp \ssend{c_1}{} \send{o}{1} + \receive{c_0}{} \send{o}{0}
	\label{leader_network_term}	
\end{align}
Here the isomorphism $\sigma$ operates in the space $\{0,1\}$, mapping 1 to 0 and 0 to 1.  
Equivalently, it takes an identifier $i$ to $(i+1)mod 2$.
The output channel $o$ is special so we extend $\sigma$ to always map $o$ to itself.
Hence $P_0 = \sigma(P_1)$ and we say that $P_0$ is symmetric to $P_1$.

Now that we have shown that (\ref{leader_network_term}) is a system of processes running `the same program', we need to show that it actually solves the leader election. 
There are two possibilities: either $P_0$ notifies $P_1$ on $c_0$ first, or $P_1$ notifies $P_0$ on $c_1$ first.  
Applying the reduction rule (R-SYNC) to (\ref{leader_network_term}), the first possibility gives
\[
	 \send{o}{0} \comp \send{o}{0}	
\]
Here we see that both processes will agree in their output.  Note that no substitutions were necessary since $\send{c}{}$ is simply a\refmargin{handshake} handshake signal.  Note also that these resulting processes are \emph{not} symmetric: applying the isomorphism $\sigma$ to $P_0$ would yield $\send{o}{1}$.

If, on the other hand, $P_1$ notifies first, then again we apply (R-SYNC) to get
\[
	\send{o}{1} \comp \send{o}{1}
\]
Again, a leader has been elected.
Hence, we have given a term that successfully solves the leader election problem for symmetric systems in the special case of a two processes.  We will discuss more general leader elections in the next chapter.