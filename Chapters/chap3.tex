%!TEX root = /Users/admin/Desktop/Documents/Academic/MA 470 -THESIS/THESIS/thesis.tex
\chapter{The Synchronous \Picalc}\label{sync_and_express}
In \refsec{spisyntax}, we introduced a version of the \picalc\ which we said was made \emph{asynchronous} by its use of non-blocking send operations.  
Rather than allowing an operation to happen after a sent value has been received by some other process, a sending process simply ends.  
In \refex{exsynchronous} we gave a straightforward method of simulating synchronous sending in the asynchronous \picalc\ using acknowledgement channels.  
Also absent in our calculus was the non-deterministic choice operator, also known as the summation operator.  

We have already had a taste of the expressive power of the synchronous \picalc\ in Chapter \ref{Introduction}'s mobile phone network example.  In fact, the original calculus given by Milner, Parrow and Walker was synchronous and included the two operators --- summation and synchronous sending --- that we omitted in our asynchronous \picalc.  

Our original calculus featured synchronous receiving, but we now allow synchronous sending as well, enabling a process to continue after its output has been consumed by some receiver.  
Hence, in the synchronous \picalc, processes can be\refmargin{guarded}guarded by both receiving \emph{and} sending.  


If we have a group of guarded processes joined by the summation operator, only one of those processes will be `chosen' (non-deterministically)\index{non-determinism} to be executed.  
The rest will simply terminate without having any effect.  
The power of the synchronous \picalc\ comes when we have another group of summed processes, running in parallel, something like: \index{choice}\index{summation}
\[
	\ssend{c}{}P_1 + \ssend{d_1}{}P_2 + \receive{d_2}{}P_3 \comp \receive{c}{}R_1 + \ssend{d_3}{}R_2
\]
In the above isolated system, we actually \emph{can} know which of the processes will be chosen to run, despite the non-deterministic nature of the choice operator.  
Because both sends and receives are guarded, the $P_k$'s and $R_k$'s above can only run when their respective transmission guards complete.  
Hence, only $P_1$ and $R_1$ will be allowed to run since sending on $c$ is the only transmission with a `matching' reception on the same channel.  
Now imagine a slightly different system:
\[
	\ssend{c}{}P_1 + \ssend{d}{}P_2 \comp \receive{c}{}R_1 + \receive{d}{}R_2
\]
Here we have \emph{two} transmissions with matches, on $c$ and on $d$.  
Hence, we cannot make a guarantee about which processes are chosen.  
However, we \emph{can} say that if $P_1$ executes on the left side, $R_1$ will be chosen on the right side.  
We can say the same for $P_2$ and $R_2$.  
There is a silent, implicit sort of communication that happens between groups of parallel processes when a non-deterministic choice is made among them.  
This is a powerful feature which comes with the special operators of the synchronous \picalc.  

Now that we have gotten an idea of the synchronous \picalc' unique properties, we will turn in the next section to its formal syntax and rules of computation.

\section{Syntax and Equivalence}\label{Synchronous picalc}

\begin{insettable}
\begin{center}
\begin{tabular}{r l l}
\multicolumn{3}{c}{\emph{Action Prefixes}}\\
$\pi :=$  & $\send{n}{\tuple{V}}$ & Send\\
&$\receivenodot{n}{\tuple{X}}$ & Receive\\

&\\

\multicolumn{3}{c}{\emph{Process terms}}\\
$R :=$ & $\displaystyle\sum_{i} \pi_i.R_i$ &\multirow{2}{*}{Summation}\\[20pt]
&$R_1 \comp R_2$ & Composition\\
&$\new{n}R$ & Restriction\\
&$\pif{v_1 = v_2}\pthen R_1 \pelse R_2$ & Matching\\
&$\rec{x} R$ & Recursion\\
&\pstop & Termination\\
&\\
\end{tabular}
\caption{\emph{Terms in the synchronous \picalc}}\label{spicalcterms}
\end{center}
\end{insettable}

Note the important difference between \reffig{apicalcterms} and \reffig{spicalcterms}.  
First, we have grouped sending and receiving together as \emph{action prefixes}. 
These prefixes are made available to processes via the summation operator. 

Consider the term:
\[
	\sum_{i} \pi_i.R_i
\]
The notation $\pi_i.R_i$ requires that the action $\pi_i$ happen before the guarded process $R_i$ can be executed.  
If $R_a$ is executed in this way, then for all $j \neq a$, the capabilities for both the action $\pi_j$ and the execution of $R_j$ are lost.  
In other words, the summation ensures that only one of the guarded processes will be executed, providing a branching behavior in the logic of a term. 
Which of these branches is picked depends on which action prefix capability is exercised first.  As we saw above, we cannot always guarantee which of the action capabilities will be exercised, so we say that the summation operator is non-deterministic.\index{non-determinism}\index{summation}\index{choice}\index{guarded}

For the cases that summation is of size 1 or 2, we will use the notation $\pi.R$ and $\pi_1.R_1 + \pi_2.R_2$, respectively.  Notice that the former is the equivalent of the process terms of our asynchronous calculus.
To accommodate the new operator, we add to our congruence relation given in \refdef{Structural Equivalence} that summation is commutative (S-SUM-COMM) and associative (S-SUM-ASSOC).

A summation of size 0 is what is meant by our $\pstop$ termination process.\index{termination}  
It behaves just as it did in our asynchronous calculus.
We also add to our congruence relation the following trivially true fact, which we call (S-SUM-ID)
\begin{align*}
	R + stop \sequiv R
\end{align*}

Note also that in our action prefixes, we have made sending a\refmargin{guarded}guarded operation, which means that in the term
\[
	\ssend{n}{\tuple{V}}R
\]
$R$ will not execute until some other process receives $\tuple{V}$ along $n$.  
Receiving is also guarded, as in the asynchronous version.  

\begin{insettable_wide}
\begin{center}
	\begin{align*}
		R + Q\ &\   \sequiv\ Q + R && \text{\tiny{(S-SUM-COMM)}}\\
		(P + Q) + R\ &\   \sequiv\ P + (Q + R) && \text{\tiny{(S-SUM-ASSOC)}}\\
		R + stop\ &\   \sequiv\ R && \text{\tiny{(S-SUM-ID)}}\\
		P \comp Q\ &\  \sequiv\  Q \comp P && \text{\tiny{(S-COMP-COMM)}}\\
	 	(P \comp Q) \comp R\ &\ \sequiv\ P \comp (Q \comp R) && \text{\tiny{(S-COMP-ASSOC)}}\\
		P \comp \pstop\ &\ \sequiv\ P && \text{\tiny{(S-COMP-ID)}}\\
		\new{c} \pstop\ &\ \sequiv\ \pstop && \text{\tiny{(S-REST-ID)}}\\
		\new{c}\new{d} P \ &\ \sequiv\ \new{d}\new{c} P && \text{\tiny{(S-REST-COMM)}}\\
		\new{c}(P \comp Q)\ &\ \sequiv\  P \comp \new{c}Q\text{, if } c\not\in \mbox{\emph{fi}}(P) && \text{\tiny{(S-REST-COMP)}}
	\end{align*}
	\caption{\emph{Structural equivalence axioms in the synchronous \picalc}}\label{spicalcaxioms}
\end{center}\end{insettable_wide}\index{structural equivalence}

\section{Semantics}
We are now ready to give a description of the computation behavior.  
As we might expect, it does not differ hugely from computation in the asynchronous calculus.  
In the reduction rules, the only changes are to make room for the summation operator.
Because only one process gets chosen from a group of summed processes, we need two `matching' sender and receiver terms each running in parallel.  This behavior is described by (R-SYNC).  
It expresses the communication step where the processes have transmitted their value.  Hence, we perform the appropriate substitution to the receiving process, run the guarded processes and terminate all the other terms in the sum.
\begin{insettable}
\begin{center}\begin{tabular}{rll}
	$\ssend{c}{\tuple{V}} P + Q \comp \receive{c}{\tuple{X}}R + B$\ &\  $\pred\  P \comp R\subst{\tuple{V}}{\tuple{X}}$ & \tiny{(R-SYNC)}\\
	$\rec{x}R$\ &\  $\pred\  R\subst{\rec{x}R}{x}$ & \tiny{(R-REP)}\\
	$\pif{v = v}\pthen P \pelse Q$\ &\ $\pred\ P$ & \tiny{(R-EQ)}\\
	$\pif{v_1 = v_2}\pthen P \pelse Q$\ &\ $\pred\ Q$ \ \ (where $v_1\neq v_2$)& \tiny{(R-NEQ)}\\
	\multicolumn{2}{c}{\hspace{4.5em}$\underline{P\sequiv P', P \pred Q, Q\sequiv Q'}$} & \multirow{2}{*}{\tiny{(R-STRUC)}}\\
	\multicolumn{2}{c}{\hspace{4.5em}$P'\pred Q'$}
\end{tabular}
\caption{\emph{Reduction rules for the synchronous \picalc}}\label{spireducs}
\end{center}
\end{insettable}\index{reduction}

\begin{example}{syncembedding}
	It is not hard to show that the synchronous \picalc\ can model the asynchronous version.  
To see why, first note that asynchronous sending can be encoded simply by $\ssend{n}{\tuple{V}}\pstop$.  
This we will abbreviate with the familiar notation $\send{n}{\tuple{V}}$.  
As we noted above, the summation notation allows for a single guarded process.  
If we limit ourselves to these single summations and limit all sending to be of the form $\ssend{n}{\tuple{V}}\pstop$, then we have the asynchronous \picalc.  
To see why the reduction semantics are compatible, consider the following which shows that (R-COMM) of \refdef{Reduction} can be considered to be a special case of (R-SYNC).\\

\noindent First, we use (S-SUM-ID):
\[
	\ssend{c}{\tuple{V}} \pstop + \pstop \comp \receive{c}{\tuple{X}}R + \pstop \sequiv \ssend{c}{\tuple{V}}\pstop \comp \receive{c}{\tuple{X}}R
\]
Using (R-SYNC) enables the action:
\[
	\ssend{c}{\tuple{V}}\pstop \comp \receive{c}{\tuple{X}}R \pred\  (\pstop \comp R)\subst{\tuple{V}}{\tuple{X}}
\]
We now use (S-COMP-ID):
\[
	(\pstop \comp R)\subst{\tuple{V}}{\tuple{X}}\sequiv R\subst{\tuple{V}}{\tuple{X}}
\]
Thus, we can now conclude, using (R-STRUC):
\[
	\send{c}{\tuple{V}} \comp \receive{c}{\tuple{X}}R \pred\  R \subst{\tuple{V}}{\tuple{X}}
\]
	It should be evident from our presentation of synchronous action rules below that they needn't be shown to be a general version of the asynchronous rules --- they are compatible simply by ignoring the extra rule and by using our encoding of asynchronous sending as $\ssend{n}{\tuple{V}}\pstop$.
\end{example}

\begin{insettable_wide}
	\begin{center}\begin{tabular}{rllll}
 		$\receive{c}{\tuple{X}}R$ & \evolves{\receives{c}{\tuple{V}}} & $R$\subst{\tuple{V}}{\tuple{X}} & & \tiny{(A-IN)}\\
		$\ssend{c}{\tuple{V}}R$ & \evolves{\sends{c}{\tuple{V}}} & $R$ & & \tiny{(A-OUT)}\\
		$\rec{x}R$ & \evolves{\tau} & $R\subst{\rec{x}R}{x}$ & & \tiny{(A-REP)}\\
		$\pif{v=v} \pthen P \pelse Q$ & \evolves{\tau} & $P$ & & \tiny{(A-EQ)}\\[10pt]
		$\pif{v_1=v_2} \pthen P \pelse Q$ & \evolves{\tau} & $Q$ & $v_1 \neq v_2$ & \tiny{(A-NEQ)}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P_i \evolves{\pi_i} P_i'}$} &  & \multirow{2}{*}{\tiny{(A-SUM)}}\\
		\multicolumn{3}{c}{$\displaystyle\sum_{i}\pi_i.P_i \evolves{\pi_i} P_i'$}\\[18pt]
		
		\multicolumn{3}{c}{$\underline{P \evolves{\pi} P'}$} & \multirow{2}{*}{\footnotesize{$\textstyle \mbox{\emph{bn}}(\alpha) \cap \mbox{\emph{fn}}(Q) = \emptyset$ }} & \multirow{2}{*}{\tiny{(A-COMP)}}\\
		\multicolumn{3}{c}{$P\comp Q \evolves{\alpha} P'\comp Q$}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P \evolves{\alpha} P'}$} & \multirow{2}{*}{\footnotesize{$\textstyle b \not \in n(\alpha)$ }} & \multirow{2}{*}{\tiny{(A-REST)}}\\
		\multicolumn{3}{c}{$\new{b} P \evolves{\alpha} \new{b} P'$}\\[10pt]

		\multicolumn{3}{c}{$\underline{P\evolves{\exports{\tuple{B}}\sends{c}{\tuple{V}}} P'}$} & \multirow{2}{*}{\footnotesize{$n \neq c, n\text{ is in }\tuple{V}$ }}& \multirow{2}{*}{\tiny{(A-OPEN)}}\\
		\multicolumn{3}{c}{$\new{n}P \evolves{\exports{n,\tuple{B}}\sends{c}{\tuple{V}}} P'$}\\[10pt]
		
		\multicolumn{3}{c}{$\underline{P\evolves{\receives{c}{\tuple{V}}} P',\ Q \evolves{\exports{\tuple{B}}\sends{c}{\tuple{V}}} Q'}$} & \multirow{2}{*}{\footnotesize{$\textstyle \exports{\tuple{B}}\cap \mbox{\emph{fn}}(P) = \emptyset$ }} & \multirow{2}{*}{\tiny{(A-COMM)}}\\
		\multicolumn{3}{c}{$P\comp Q \evolves{\tau} \new{\tuple{B}}(P'\comp Q')$}\\[10pt]
	\end{tabular}	
	\caption{\emph{Action rules for the synchronous \picalc}}\label{spiacts}
\end{center}
\end{insettable_wide}\index{action}
Neither do the action rules for the synchronous \picalc\ differ significantly from \hyperref[apiactionrules]{those} in the asynchronous version.  
As we would expect, (A-OUT) has been modified to express synchronous sending. 
It now evolves over output to a process $R$ and not simply to the termination process $\pstop$:
\[
	\ssend{c}{\tuple{V}}R \evolves{\sends{c}{\tuple{V}}} R	
\]

We have also added a new rule, (A-SUM). It describes the action of the summation operator much as (A-COMP) describes the action of the composition operator.
\begin{center}
	$\underline{P_i \evolves{\pi_i} P_i'}$\\
	$\displaystyle\sum_{i}\pi_i.P_i \evolves{\pi_i} P_i'$\\
\end{center}
(A-SUM) expresses the fact that, when a single guarded process can evolve in one step to a process $P_i'$ due to an action $\pi_i$, then it can also do so as part of a summation.
Notice that unlike (A-COMP), (A-SUM) does not need to be careful about capturing names. 
Since no other processes are run by the summation, we needn't worry about causing a naming issue when we possibly export terms by running the action $\alpha$.

\section{Extended Example: Leader Elections}\label{secleaderelecs}
Leader elections, a classic problem in distributed systems, are a good example of the power of the synchronous \picalc.\index{leader elections}
A leader election is a system where a group of processes, each with a unique identifier (an integer, perhaps), must agree on the selection of a `leader' process.
The processes each choose a process to be their leader by sending their choice on a given output channel $o$.   
The choice consists of a channel name $c_i$ representing their choice for $P_i$ .  

Ideally, we want each of the processes to operate using the same `program', without any preference or priority hard-coded into that program.
One means of specifying that the programs are the same is by the concept of \defmargin{symmetry}.
We say that a system is symmetric if its processes are equivalent under structural equivalence and any systematic renaming of their identifiers.
The formal definition of `a systematic renaming' is given by Palamidessi \cite{palam03} as an automorphism $\sigma$.
It maps exposed channels in the processes to other free channels, and maps processes in our system to other processes.\refmargin{exposed channels}
For example, consider the following symmetric system:
\begin{align}
	P_0\comp P_1 \pdef \ssend{c_0}{} \send{o}{c_0} + \receive{c_1}{} \send{o}{c_1} \comp \ssend{c_1}{} \send{o}{c_1} + \receive{c_0}{} \send{o}{c_1}
	\label{leader_network_term}	
\end{align}
Here there are only two possible automorphisms: the identity function and one that maps $P_0$ to $P_1$ (and vice versa) and $c_0$ to $c_1$ (and vice versa).  
If we call the later case $\sigma$, we have that $P_1 = \sigma(P_0)$. 
Note that we consider the output channel $o$ as a special case that is not operated on by the automorphism.
Hence we can say that the leader election system $P_0 \comp P_1$ is symmetric.

Now that we have shown that (\ref{leader_network_term}) is a system of processes running `the same program', we need to show that it actually solves the leader election. 
There are two possibilities: either $P_0$ notifies $P_1$ on $c_0$ first, or $P_1$ notifies $P_0$ on $c_1$ first.  
Applying the reduction rule (R-SYNC) to (\ref{leader_network_term}), the first possibility gives
\[
	 \send{o}{c_0} \comp \send{o}{c_0}	
\]
Here we see that both processes will agree in this ``election'' --- that is, they output the same value.  Note that no substitutions were necessary since $\send{c}{}$ is simply a\refmargin{handshake} handshake signal.  Note also that these resulting processes are \emph{not} symmetric: applying the automorphism $\sigma$ to the residual $\send{o}{c_0}$ of $P_0$ would yield $\send{o}{c_1}$.

If, on the other hand, $P_1$ notifies first, then again we apply (R-SYNC) to get
\[
	\send{o}{c_1} \comp \send{o}{c_1}
\]
Again, a leader has been elected.
Hence, we have given a term that successfully solves the leader election problem for symmetric systems in the special case of two processes.  We will discuss more general leader elections in the next chapter.