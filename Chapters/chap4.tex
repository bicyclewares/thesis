\chapter{Synchronicity and Distributed Systems}\label{sync_and_dist_sys}
Can we represent the expressive communication power of the synchronous \picalc\ using only our asynchronous \picalc?  That is, using the simulations given in Examples \ref{exsynchronous} and \ref{exsummation} (or perhaps some similar but more complicated approach) can we fully capture the implicit `communication' (see the introduction to Chapter \ref{sync_and_express}) between non-deterministically chosen processes discussed above?  In this chapter, we will explore the surprising complexity of this question and some of its implications in implementing the synchronous \picalc\ on distributed systems.  



\section{Separation Results}\label{Separation Results}
When trying to compare the expressive power of different calculi, one approach is to provide explicit encodings from one language to another as we did in \refex{syncembedding}.
We say that we are trying to encode a \defmargin{source language} into the terms of a \defmargin{target language}. If we succeed, we have shown that the target language is at least as expressive as the source.  
Hence, in example \refex{syncembedding} we showed that the synchronous calculus is at least as expressive as the asynchronous calculus by giving an encoding from the asynchronous to the synchronous.  
We use the notation
\[
	\encode{P} \defequals Q
\]
to mean that P in the source language is encoded by Q in the target language.

To prove a separation result between languages, it is enough to show that there are problems that are solvable in the source language that are not solvable in the target language.
In \refsec{secleaderelecs}, we showed that a special case of the leader election problem for symmetric systems was solvable in the synchronous \picalc.
In fact, the synchronous \picalc\ is capable of solving this problem in general.
Catuscia Palamidessi shows this in \cite{palam03}.
She then goes on to show that the asynchronous \picalc\ is \emph{not} capable of solving the leader election problem in symmetric systems.
 
The latter result stems directly from the lack of the choice operator: without it, symmetric processes have no way to pick a leader non-deterministically without potentially disagreeing with one another.  
It is only through the implicit communication underlying the choice operator that synchronous processes are able to agree on a leader.

We will not give the full argument here, but we will try to give a sketch of its important properties and will also look at an illustrative example in \refsec{failedencoding}.  
Palamidessi's proof lies on an important property enjoyed by only the \emph{asynchronous} \picalc:  
\emph{confluence}, which comes from lemma 4.1 from Palamidessi's proof.
\begin{quote}
	\textbf{Lemma 4.1} \emph{Let P be a process of the asynchronous \picalc.  Assume that P can make two transitions $P \evolves{\alpha_s} Q$ and $P \evolves{\alpha_r} Q'$, where $\alpha_s$ is a send action while $\alpha_r$ is a receive action.  Then there exists a process R such that $Q \evolves{\alpha_r} R$ and $Q' \evolves{\alpha_s} R$.}
\end{quote}
Our processes may start out symmetrical, but as we showed in \refsec{secleaderelecs} in the synchronous calculus the processes must break their original symmetry in order to elect a leader.  
The issue with the asynchronous calculus is that if a process has two options available to it (say to vote or wait for the other process to vote first) and it chooses one, confluence guarantees that we can still make the action representing the other choice, bringing us to a state that is the same as if we had performed the actions in the opposite order.  
Consider trying to encode the symmetric election system from \refsec{secleaderelecs}:
\begin{align}
	P_0\comp P_1 \pdef \ssend{c_0}{} \send{o}{0} + \receive{c_1}{} \send{o}{1} \comp \ssend{c_1}{} \send{o}{1} + \receive{c_0}{} \send{o}{0}
	\label{leader_network_term2}	
\end{align}
Whatever our encoding looks like, $\encode{P_0}$ will have a choice between voting 0 our waiting - call these $\alpha_s$ and $\alpha_r$ respectively.  
Suppose it chooses $\alpha_s$.  
Then $\encode{P_1}$ must make the opposite choice $\encode{\alpha_r}$ in order for them to agree on a leader.  
But by confluence $\encode{P_0}$ still has the capability for $\alpha_r$ and $\encode{P_1}$ the capability for $\encode{\alpha_s}$.  
After these actions, $\encode{P_0 \comp P_1}$ is no different than if $\encode{P_0}$ had made the opposite choice first!  Hence, no matter what the processes do, they will not be able to break their initial symmetry and are unable to elect a leader.

Using her separation results, Palamidessi goes on to give a useful set of requirements that formally separate the two calculi.
The first of those requirement is \defmargin{uniformity}, which means that:
\todo{better labeling of these proerties?}
\begin{align}
	\encode{\sigma(P)} &= \sigma(\encode{P})\label{unif1}\\
	\encode{P\comp Q} &= \encode{P} \comp \encode{Q}\label{unif2}
\end{align}
Rule (\ref{unif1}) simply states that an arbitrary renaming function $\sigma$ is not violated in the process of the encoding.  
That is, if we $\sigma$-rename a process $P$ and then encode it we get the same result as if we encode it and then $\sigma$-rename it.  
Because our encoding is required to preserve $\sigma$, a symmetric term in the source language will still be symmetric in the target language.

Rule (\ref{unif2}) is related to the requirements of a distributed system.  
Parallel processes really should just map to parallel processes, with no top level `manager' process or the like to aide the encoding.  
That is, we wouldn't want to encode $P \comp Q$ to something like 
\[
	P \comp Manager \comp Q
\]
Hence, in uniformity, Palamidessi has built symmetry and distributivity into a general requirement on the encoding.

The other requirement is on \defmargin{reasonability} and is on the target language's semantics.  
Reasonability to Palamidessi means that the language can distinguish between two processes when their actions are different on a certain given channel.  
This requirement essentially builds the requirements of the leader election problem into a criteria for the language.  
Thus, reasonability of a target language means the capability of semantically distinguishing an electoral system, where all actions on the output channel to be the same, from a non-electoral system, where actions on the output channel differ or never happen at all.
\todo{come up with an implement a short hand for the calculi: SPI and API?}

Given these results, it should come as no surprise that the implementation of synchronous calculi on distributed systems is a thorny issue.  
On the one hand, its additional operators allow us to model many problems more naturally and easily.  
On the other, communication in a distributed system is asynchronous in nature and any synchronous communication must implemented as a layer on top of an asynchronous base.  
How can we reconcile the requirements of implementing a distributed system with Palamidessi's result, which indicates that there are important problems that cannot be solved without the full generality of the synchronous calculus?  
Is such a calculus even implementable on distributed systems?
We will look below at two encodings that attempt to do so in the proceeding sections.  
Both encodings based on those given by by Uwe Nestmann in \ref{nest00}.

\section{Encoding Choice}\label{failedencoding}
Our first encoding limits the source language a bit: we do not allow processes in a given summation to be a mix of terms with receive prefixes and send prefixes.  
A group of summed terms must either be all `receivers' or all `senders'.  
We will call this limitation on summation \emph{separated choice}.

With this limitation in place, we can provide a good encoding.  
The basic idea for this encoding is similar to that of \refsec{secmemcells}, where we used a lock to protect a memory cell from multiple clients trying to use the cell at once.  
Here again, we want to provide mutually exclusive access to something --- only instead of the ability to access to the cell we are providing the chosen process the ability to execute.  
The chosen process is the only one that gets run, so once the process runs the lock never become unlocked.  
In order to proceed, a receiver term in one summation depends on a sender term in another summation proceeding as well.
Thus we need \emph{two} locks: acquiring one ensures that a receiving process has been chosen in one summation, and acquiring the other lock ensures that the corresponding sending process in the other summation is also chosen to run.

To model synchronous sending, we use an acknowledgement channel $ack$ in send actions as in \refex{exsynchronous}.  On the other side of a transmission, our receive action uses our `double lock' to implement choice and is also responsible for sending the correct acknowledgement depending on the results of trying to get that lock.  Note that we instantiate one shared lock for each summation, as specified by the encoding of summation:
\[
	\encode{\displaystyle\sum_{i\in \{1,..,n\}} \pi_i.R_i} \defequals \new{l}(\send{l}{\ptrue} \comp \displaystyle\prod_{i\in \{1,..,n\}}\encode{\pi_i.R_i}_l)
\]
The notation $\prod$ means composition.  We have parameterized the encoding $\encode{\pi_i.R_i}$ with the created lock channel $l$ so that all the terms in a sum have access to the same lock.  Thus, a group of summed senders will have one lock, which we will call a \emph{remote} lock $r$, and a group of summed receivers will have another lock, which we will call a \emph{local} lock $l$.

Now we give the encoding for a sending process:
\begin{equation*}\begin{split}
	\encode{\ssend{c}{\tuple{V}}P}_r \defequals & \new{ack}(\send{c}{r,ack,\tuple{V}} \comp \receive{ack}{x} \pif{x=\ptrue} \pthen \encode{P} \\
	&\pelse (\pif{x=\pretry} \pthen \send{c}{r,ack,\tuple{V}} \pelse \pstop))
\end{split}\end{equation*}
This encoding simply sends on $c$ the value, lock, and acknowledgement channel and then waits to receive the acknowledgement.  
If it receives \ptrue it runs the encoding of $P$ and if it receives \pretry then it resends the input, which we will explain below.  Otherwise it has received \pfalse and it terminates accordingly.

Our receiver process encoding is simply a recursive listener that tries the lock when it get input on $c$:
\begin{equation*}\begin{split}
	\encode{\receive{c}{\tuple{X}}P}_l \defequals & \rec{q}\receive{c}{r,ack,\tuple{X}} (l,r)?d-lock.\encode{P}
\end{split}\end{equation*}
We use the notation $(l,r)?d-lock.P$ as shorthand for our double lock.
Note that the recursive variable $q$ defined above is actually called in the body of the double lock.
So what does this double lock look like?  Without further ado:
\begin{equation}\begin{split}
	\receive{l}{x}(&\pif{x=\ptrue}\\
	&\pthen \receive{r}{y} (\pif {y=\ptrue}\\
	&\ \ \ \ \ \ \pthen \send{l}{\pfalse} \comp \send{r}{\pfalse} \comp \send{ack}{\ptrue} \comp P\\
	&\ \ \ \ \ \ \pelse \send{l}{\ptrue} \comp \send{r}{\pfalse} \comp \send{ack}{\pfalse} \comp q)\\
	&\pelse\send{l}{\pfalse} \comp \send{ack}{\pretry})\\
\end{split}\end{equation}%note: it was really late when i did this.  sorry....
Let's go through the cases to see how this works.  
First we check the local lock, and if it's available, we check the remote lock (note that this order is useful in the real-world case where a remote lock is more expensive to query than a local one).  
If both are available, first we send \pfalse to both locks to make sure no processes in either summation can acquire the lock.  
Then, we run $P$ and send \ptrue to $ack$ so the matching sender in the other summation will run.

If the local lock is acquired but the remote lock isn't, we make sure that the local lock is still available by sending \send{l}{\ptrue} and that the remote lock is \emph{not} made available by sending \send{l}{\pfalse}.  
We also need a recursive call to $q$ to ensure we continue to poll the lock ourselves.  
The fact that the remote lock is not available means another sender has already run (i.e. it was sent \ptrue on its acknowledgement channel by some receiver) so we need to make sure \emph{this} sender is sent \pfalse on its acknowledgement channel.

Finally, if we fail to acquire a local lock, we send \pfalse to the lock to ensure no one else gets it either, and then we send the acknowledgement channel the special message \pretry so that sender still has a chance to run, since the remote lock might still be available.

Though this clever double lock successfully encodes our source language restricted to separated choice, it does not effectively encode the full synchronous \picalc.  
For example, our two-party electoral system breaks the check and balance system.  Here is that system again:
\begin{align}
	P_0\comp P_1 \pdef \ssend{c_0}{} \send{o}{0} + \receive{c_1}{} \send{o}{1} \comp \ssend{c_1}{} \send{o}{1} + \receive{c_0}{} \send{o}{0}
\end{align}
Notice that the encoding of the senders will both send, on $c_0$ and $c_1$ respectively.  
Now suppose that both receivers are running and have acquired their local lock.  
Since one's local lock is the other's remote lock, both will be trapped in a circular wait, with no hope of exiting the deadlock.  
As per Palamidessi's results, neither process can break out of its symmetry and thus neither will ever have a chance to vote.

\section{A `Bakery' Algorithm}
We showed in the last section that the asynchronous \picalc\ is fully capable of encoding the summation and synchronous send operators when limited to separated choice.
However, to truly encode the synchronous \picalc, we need to derive a way to break out of symmetry.

In fact, this is not a new problem in computer science and many solutions have been proposed, though all of them violate Palamidessi's encoding criteria to some extent.
Palamidessi herself gives a probabilistic encoding \cite{palam01} which does not break uniformity \refmargin{uniformity} but is not reasonable \refmargin{reasonability} since, though it succeeds with probability 1, it cannot distinguish between the freak case divergence and a successful election.
Another route is to violate symmetry by comparing process ids.
In this case, processes can make asymmetrical decisions by simply comparing their ids and deciding accordingly.

Better yet is a symmetric --- though still not uniform --- variation on Lamport's classic Bakery algorithm \todo{citation for lamport} by given in \cite{nest00}.
Instead of statically encoding the asymmetry in process ids, Lamport's algorithm has processes dynamically obtain a number from a number server much as a patron might when entering the queue at a bakery counter.
Because this asymmetry is dynamically determined, the real-world implications on fairness are acceptable but the processes can always elect a leader.
Requiring a number server also breaks uniformity, since it requires a top-level number server process, but again in a real-world distributed system the cost of running a single number server to provide ids to processes is not serious.  
Below, we assume the existence of a top level number server which is really just a shared channel $d$ initialized with some integer value.  
Our encoding of the summation operator simply grabs the current number from the server and then sends the next integer value for the next summation operator.  
Other than that it is the same as our last encoding:
\[
	\encode{\displaystyle\sum_{i\in \{1,..,n\}} \pi_i.R_i}^d \defequals \receive{d}{n}(\send{d}{n+1} \comp \new{l}(\send{l}{\ptrue} \comp \displaystyle\prod_{i\in \{1,..,n\}}\encode{\pi_i.R_i}^d_{n,l}))
\]
Notice that the encoding passes on the number \emph{and} number server to the summed terms.  
We pass on the latter because some component process of $R_i$ might contain a summation term itself, in which case it will need access to $d$.

The only change in the encoding of the send operator is that we now send $n$ on $c$ as well:
\begin{equation*}\begin{split}
	\encode{\ssend{c}{\tuple{V}}P}^d_{n,l} \defequals & \new{ack}(\send{c}{n,l,ack,\tuple{V}} \comp \receive{ack}{x} \pif{x=\ptrue} \pthen \encode{P}^d \\
	&\pelse (\pif{x=\pretry} \pthen \send{c}{n,l,ack,\tuple{V}} \pelse \pstop))
\end{split}\end{equation*}

The basic new idea of Nestmann's application of the Bakery algorithm is that our leader election deadlock could have been avoided if the receiver processes had checked their locks in a different order.  
Since both receiver processes checked their local locks and then the remote lock, each held the lock the other was hoping to acquire.  
If each process had a number, they could simply compare numbers and have the one with the lower number check its \emph{local} lock first.  
The process with the higher number would check its \emph{remote} lock first.  
If two processes have the same number, then that means that they are of the same summation and that \emph{neither} should run (without the help of some matching transmission in another summation).  
Here we give the encoding for the receiver:
\begin{equation*}\begin{split}
	\encode{\receive{c}{\tuple{X}}P}^d_{n,l} \defequals & \rec{q}(\receive{c}{m,r,ack,\tuple{X}}(\\
	&\pif{n=m} \pthen (\send{ack}{\pretry}\comp q) \pelse(\\
	&\pif{n<m} \pthen (l,r)?d-lock.\encode{P}^d \pelse (r,l)?d-lock.\encode{P}^d)))\\
\end{split}\end{equation*}
The double lock $(l,r)$ is just as before, but the other double lock is slightly different.  It expresses that $r$ is checked before $l$.  We use $(r,l)?d-lock.P$ to denote:
\begin{equation}\begin{split}
	\receive{r}{x}(&\pif{x=\ptrue}\\
	&\pthen \receive{l}{y} (\pif {y=\ptrue}\\
	&\ \ \ \ \ \ \pthen \send{l}{\pfalse} \comp \send{r}{\pfalse} \comp \send{ack}{\ptrue} \comp P\\
	&\ \ \ \ \ \ \pelse \send{l}{\pfalse} \comp \send{r}{\ptrue} \comp \send{ack}{\pretry})\\
	&\pelse\send{r}{\pfalse} \comp \send{ack}{\pfalse}\comp q)\\
\end{split}\end{equation}%note: it was really late when i did this.  sorry....
Because the order of the conditions is reversed, we've had to reverse the order of the else statements too.  The case of successfully getting both the remote and local locks is the same as in (l,r).  If the process only gets the remote lock it tells the remote sender to keep trying and continues to make the remote lock available.  If on the other hand it fails to get the remote lock, it restarts the receiver and makes sure to tell the remote sender its failed.

Note that Nestmann's `implementation' of the synchronous \picalc\ does not pay attention to the efficiency questions that would be crucial in a real system.  Perhaps its worst property is that in the $n=m$ case there is a possible divergence.  If two terms (a sender and a receiver) in the same summation are communicating with one another, our $n=m$ case has them both retry.  Thus, they might continue trying forever.  Though this represents potentially divergent behavior, it is important to note that this is not the same as a live-lock: the processes will run forever without progressing, but only because they are both in a summation that is unable to progress anyway.  However, it still represents a violation of reasonability since it is possible (though unlikely) that the two might continue communicating forever, never allowing for the computation step of sending the leader votes to the output channel $o$\footnote{Though we will not show it here, Nestmann does goes on in \ref[nest00] to give the same encoding, using a slightly enhanced target language, that does not have this divergence problem.}.  Hence, while Nestmann's encoding violates both of Palamidessi's criteria to some extent, it nevertheless provides the full behavior of the synchronous in a way that could easily be implemented on a distributed system using only asynchronous primitives.